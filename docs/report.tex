%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letter, 12pt, conference]{ieeeconf}

\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% Reduce space between figures and text
\setlength{\textfloatsep}{5pt}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{nth}
\usepackage{pgfplots}
\usepackage{titlesec}
\usepackage[backend=biber,style=ieee]{biblatex}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\todo}{\colorbox{red}{TODO}}
\newcommand{\todocite}{\colorbox{red}{CITE}}

\addbibresource{references.bib}
\renewcommand*{\bibfont}{\small}

\title{\LARGE \bf Practical Analysis of Hybrid Sorting Algorithms}
\author{Joshua Arulsamy}

\author{\parbox{3 in}{
\centering
Joshua Arulsamy\\
University of Wyoming\\
{\tt\small jarulsam@uwyo.edu}}}

\begin{document}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}
\nocite{*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

	\todo

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Most applications require data to be arranged in a certain order. The procedure
of reordering data according to some comparator is called sorting. Sorting can
be found in nearly every computer system today. From databases to payment
processing systems, data is constantly being sorted. The near omnipresent need
for performant sorting implementations has resulted in most major programming
languages including a type-generic sorting function within their standard
libraries. These standard implementations can use a variety of algorithms
internally, such as Quicksort, Mergesort, and many others (\todocite), however,
rarely does a single algorithm perform exceptionally well. Often, these
algorithms are paired with one or more other sorting algorithms to offer
sizeable performance benefits. These hybrid algorithms include some tuneable
parameters, such as when to switch from one sorting algorithm to another. While
these hybridized algorithms have varying space and time complexities, their
actual performance in real-world scenarios is rarely formally examined, and the
values for these parameters are often chosen arbitrarily by the developers. This
paper analyzes a wide variety of sorting algorithm implementations across
varying system architectures to determine optimal parameter configurations for
hybrid sorting algorithms. This paper also proposes a new performant standard
sorting implementation for GNU's C standard library.

\section{BACKGROUND}

Substantial effort has been spent on the analysis of sorting algorithms.
Typically, two major characteristics are considered when studying sorting
algorithms from a purely theoretical standpoint; space and time complexity.
Existing analysis of popular sorting algorithms, such as Quicksort and
Mergesort, place substantial emphasis on their excellent asymptotic complexity
in the average case(\todocite). However, asymptotic complexity alone does not
describe the practical performance of these algorithms. While Mergesort may have
a better asymptotic time complexity in the worst case than insertion sort (Table
\ref{fig:time_complexity_table}), for small inputs, insertion sort typically
outperforms Mergesort by avoiding the overhead of recursive calls.

\begin{table}[ht]
	\centering
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{|c|lll|}
			\hline
			\textbf{Algorithm} & \multicolumn{3}{c|}{\textbf{Time Complexity}}                                                                                 \\ \hline
			                   & \multicolumn{1}{l|}{\textbf{Best}}            & \multicolumn{1}{l|}{\textbf{Average}}         & \textbf{Worst}                \\ \hline
			Quicksort          & \multicolumn{1}{l|}{$\Omega(n\log{(n)})$}     & \multicolumn{1}{l|}{$\Theta(n\log{(n)})$}     & $\mathcal{O}(n^{2})$          \\ \hline
			Mergesort          & \multicolumn{1}{l|}{$\Omega(n\log{(n)})$}     & \multicolumn{1}{l|}{$\Theta(n\log{(n)})$}     & $\mathcal{O}(n\log{(n)})$     \\ \hline
			Heapsort           & \multicolumn{1}{l|}{$\Omega(n\log{(n)})$}     & \multicolumn{1}{l|}{$\Theta(n\log{(n)})$}     & $\mathcal{O}(n\log{(n)})$     \\ \hline
			Bubble Sort        & \multicolumn{1}{l|}{$\Omega(n)$}              & \multicolumn{1}{l|}{$\Theta(n^{2})$}          & $\mathcal{O}(n\log{(n)})$     \\ \hline
			Insertion Sort     & \multicolumn{1}{l|}{$\Omega(n)$}              & \multicolumn{1}{l|}{$\Theta(n^{2})$}          & $\mathcal{O}(n^{2})$          \\ \hline
			Tree Sort          & \multicolumn{1}{l|}{$\Omega(n\log{(n)})$}     & \multicolumn{1}{l|}{$\Theta(n\log(n))$}       & $\mathcal{O}(n^{2})$          \\ \hline
			Shell Sort         & \multicolumn{1}{l|}{$\Omega(n\log{(n)})$}     & \multicolumn{1}{l|}{$\Theta(n(\log(n))^{2})$} & $\mathcal{O}(n(\log(n))^{2})$ \\ \hline
		\end{tabular}}
	\caption{Sorting algorithm asymptotic time complexities \todocite}
	\label{fig:time_complexity_table}
\end{table}

Most of these algorithms have not been sufficiently benchmarked in real world
scenarios to determine an optimal implementation for common inputs. Evaluating
specific scenarios where an algorithm performs well may lead to better sorting
function implementations by combining several algorithms or excluding algorithms
in specific situations. Since sorting is used so ubiquitously, even incremental
improvements to standard library sorting implementations can broadly influence
the performance of many different applications. This marks an area in obvious
need of further investigation, and such an analysis does not presently exist.

\section{EXPERIMENTAL SETUP}

A small subset of algorithms were chosen for evaluation. Primarily, algorithms
utilized within standard library implementations are of particular interest,
since they are typically highly performant in the general case. Other
complementary algorithms which perform exceptionally well in specific scenarios
were also evaluated. Since this analysis is focused on analyzing and improving
existing implementations, general purpose sorting function were taken from
various standard libraries, such as GNU's libc\parencite{glibc} and musl
libc\parencite{musl_libc}. It is a priority to maintain the applicability of any
optimizations discovered as a result of this evaluation to existing standard
libraries, so any non-comparison based sorting algorithm, such as Radix sort or
Bucket sort were excluded.

\subsection{General Purpose Algorithms}

Of the well known sorting algorithms, several are used for general sorting
tasks, such as Quicksort, Mergesort, Heapsort, and Introsort. Quicksort has been
a staple algorithm for decades. It is well known to be easy to implement and
works well for a variety of input data. Quicksort requires a constant amount of
extra memory independent of the size of the input, making it an obvious choice
in resource constrained environments. However, in the advent of modern computing
where using auxiliary memory to improve performance is much more common,
Quicksort tends to fall behind other algorithms. Quicksort's performance is also
primarily dictated by the selection of a suitable pivot value. Poor pivot
selections can lead to worse performance. Simple implementations usually choose
the left-most value of the input as the pivot, however this offers poor
performance in many circumstances such as if the input is reverse sorted. So
most implementations use a slightly more complex and costly pivot selection
process such as \textit{median-of-three}\todocite. The increased memory
availability of modern systems and the difficulty of good pivot selection within
Quicksort has motivated many standard libraries -- such as GNU's libc -- to
default to using Mergesort, falling back to Quicksort only when memory
requirements are too high\parencite{glibc}. Such implementations are typically
more performant in most cases, since Mergesort can leverage extra memory for
better performance, and maintain reliability even in memory constrained
environments by falling back to Quicksort whenever necessary.

\subsection{Specialized Algorithms}

A few algorithms used in specialized circumstances are also evaluated, most
notably, network sort, insertion sort, and shell sort. These algorithms are
often regarded as the best sorting algorithms for extremely small inputs. Since
their asymptotic complexity is substantially worse than a general purpose
sorting algorithm such as Quicksort or Mergesort their runtime typically grows
exponentially as the input size increases (Table
\ref{fig:time_complexity_table}). However, for small inputs, these algorithms
are highly performant since they avoid the overhead of more complicated
algorithms.

\subsubsection{Sorting Networks}

Sorting networks, also commonly referred to as comparator networks, are designed
to sort fixed numbers of items. Unlike other sorting algorithms, networks cannot
handle arbitrarily large inputs. However, even with these many constraints,
sorting networks are highly performant in instances where the input is extremely
small.

% These algorithms are prime candidates for hybridization with general
% purpose algorithms. For example, combining insertion sort with Mergesort can
% avoid repeated recursive calls for small arrays, granting a performance benefit.

\subsection{Proposed Algorithm}

General purpose sorting algorithms are typically difficult to optimize
uniformly. Since these implementations very often live in standard libraries,
use-case specific optimizations are not feasible. During runtime, quick,
low-overhead analysis about the input can reveal opportunities for
input-specific optimizations. More esoteric sorting methods, such as TimSort,
take advantage of such analysis during sorting to improve performance in most
real-world cases. However, for small input sizes, the overhead from such an
analysis often contributes more to the overall runtime, than even utilizing an
algorithm ill-suited for that specific input\parencite{the_basic_algorithms}.
This motivates placing significant importance on the size of the input itself.

Comparing the input size against predefined threshold values is cheap and can
help pick an algorithm. Since such comparisons are so cheap, they can be
repeated during recursive calls of other algorithms. This algorithm checks the
size of each subarray during Mergesort before switching to insertion sort for
small arrays. In instances where a subarray has less than 5 elements, a network
sorting algorithm is used instead of insertion sort. Utilizing a secondary
algorithm limits the depth of recursive calls substantially. Practically, this
reduces a significant amount of the overhead of Mergesort, even though there is
an additional size comparison during each recursive
call\parencite{the_basic_algorithms}.

\todo\ Document proc parameters

\begin{algorithm}[ht]
	\caption{Network Sort}
	\label{alg:network_sort}
	\begin{algorithmic}
		\Procedure{SORT2}{$a$, $b$}
		\State $t \gets min(a, b)$
		\State $b \gets max(a, b)$
		\State $a \gets t$
		\EndProcedure

		\Procedure{SORT3}{$a$, $b$, $c$}
		\State SORT2($a$, $b$)
		\State SORT2($b$, $c$)
		\State SORT2($a$, $b$)
		\EndProcedure

		\Procedure{SORT4}{$a$, $b$, $c$, $d$}
		\State SORT2($a$, $b$)
		\State SORT2($b$, $c$)
		\State SORT2($a$, $c$)
		\State SORT2($b$, $d$)
		\State SORT2($b$, $c$)
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
	\caption{Insertion Sort}
	\label{alg:insertion_sort}
	\begin{algorithmic}
		\Procedure{INS\_SORT}{a, n}
		\If {$n = 2$}
		\State SORT2($a[0]$, $a[1]$)
		\State \Return
		\ElsIf {$n = 3$}
		\State SORT3($a[0]$, $a[1]$, $a[2]$)
		\State \Return
		\ElsIf {$n = 4$}
		\State SORT4($a[0]$, $a[1]$, $a[2]$, $a[3]$)
		\State \Return
		\EndIf

		\For {$i \gets 1$ to $n$}
		\State $c \gets 1$
		\State $j \gets i - 1$
		\If {$a[j] > a[j]$}
		\State $tmp \gets a[i]$
		\Repeat
		\State $a[j + 1] \gets a[j]$
		\State $j \gets j - 1$
		\If {$j = 0$}
		\State $c \gets 0$
		\State break
		\EndIf
		\Until $a[j] \le tmp$
		\State $a[j + c] \gets tmp$
		\EndIf
		\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
	\caption{Merge Sort}
	\label{alg:merge_sort}
	\begin{algorithmic}
		\Procedure{msort}{$a$, $n$, $threshold$}
		\If {$n \le 1$}
		\State \Return
		\EndIf

		% \Comment{Use insertion sort if the input is small.}
		\If {$n < threshold$}
		\State ins\_sort($a$, $n$)
		\State \Return
		\EndIf

		\State $n_{1} \gets \frac{n}{2}$
		\State $n_{2} \gets n - n_{1}$
		\State $b_{1} \gets a$
		\State $b_{2} \gets \&b[n_{1}]$

		% \Comment{Recursively sort two halves.}
		\State MSORT($b_{1}$, $n_{1}$, $threshold$)
		\State MSORT($b_{2}$, $n_{2}$, $threshold$)

		% \Comment{Merge the two halves.}
		\While {$n_{1} > 0$ and $n_{2} > 0$}
		\If {$b_{1}[0] \le b_{2}[0]$}
		\State $tmp \gets b_{1}$
		\State $b_{1} \gets \&b_{1}[1]$
		\State $n_{1} \gets n_{1} - 1$
		\Else
		\State $tmp \gets b_{2}$
		\State $b_{2} \gets \&b_{2}[1]$
		\State $n_{2} \gets n_{2} - 1$
		\EndIf
		\EndWhile

		\If {$n_{1} > 0$}
		\State copy($tmp$, $b_{1}$, $n_{1}$)
		\EndIf
		\EndProcedure
		\State copy($a$, $tmp$, $n - n_{2}$)
	\end{algorithmic}
\end{algorithm}

\section{RESULTS}

\todo

\section{CONCLUSION}

\todo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% This command serves to balance the column lengths on the last page of the
% document manually. It shortens the textheight of the last page by a suitable
% amount. This command does not take effect until the next page so it should
% come on the page before the last. Make sure that you do not shorten the
% textheight too much.
% \addtolength{\textheight}{-12cm}

% \titleformat*{\section}{\fontsize{12pt}{14pt}\MakeUppercase\selectfont}
\printbibliography
\end{document}
