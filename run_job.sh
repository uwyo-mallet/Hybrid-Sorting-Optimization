#!/bin/bash

# Determine the correct number of jobs to submit, create a results directory,
# then submit all the jobs as an sbatch job array.

# DO NOT EDIT job.sbatch!
# Instead, edit the following variables, then run with ./run_job.sh
CWD=$(realpath .)                                         # Project directory.
INPUT_DIR="${CWD}/slurm.d/"                               # Dir with all commands to run (slurm.d/).
RESULTS_DIR="${CWD}/results/$(date +"%Y-%m-%d_%H-%M-%S")" # Place to store all the results.

echoerr() { printf "%s\n" "$*" >&2; }

if ! [ -d "$INPUT_DIR" ]; then
  echoerr "Cannot open $INPUT_DIR"
  exit 1
fi

mkdir -p "$RESULTS_DIR"

cd "$RESULTS_DIR" || exit 1

total_num_jobs=0

files=("${INPUT_DIR}"/*.dat)
sorted_files=($(printf "%s\n" "${files[@]}" | sort -V))

for f in "${sorted_files[@]}"; do
  num_lines=$(wc -l <"$f")
  printf "%s" "$(basename "$f") ${num_lines}: "
  sbatch --array "0-${num_lines}%50" "${CWD}/job.sbatch" "$f"
  ((total_num_jobs += num_lines))

  # Sleep per batch of jobs. Otherwise, this causes slurm to fail MANY jobs.
  sleep 300
done

printf "\n%s\n" "Total number of jobs: ${total_num_jobs}"

# Copy job details generated by jobs.py
cp "${INPUT_DIR}/job_details.json" "${RESULTS_DIR}/."

# Ensure the slurm.d/ dir is preserved
cp -r "$INPUT_DIR" "${RESULTS_DIR}/."
